# Решение итогового проекта [Yandex тренировок по DevOps](https://yandex.ru/yaintern/training/devops-training?utm_campaign=training4&utm_content=digest&utm_medium=email&utm_source=mindbox)

## Запуск решения

Решение предназначено для развертывания в [Yandex Cloud](https://cloud.yandex.ru/).

Разворачивается стандартными инструментами terraform:

```
terraform init
terraform plan
terraform apply
```

Данный terraform модуль требует ряд входных параметров.

- Обязательные:
    - **cloud_id** -  ID облака, в котором создавать ресурсы.
    - **folder_id** - ID каталога, в котором создавать ресурсы.
    - **zone** - зона доступности, в которой создавать ресурсы (все ресурсы создаются в одной зоне доступности).
    - **token** - IAM токен доступа в Yandex Cloud.
    - **dns_name** - доменное имя, на котором планируется сделать сервис доступным (необходимо для генерации корректных ssl сертификатов).
- Необязательные:
    - **sa_prefix** - имена сервисных аккаунтов должны быть уникальны в облаке. Если вы делите облако с другими пользователями, имена сервисных аккаунтов, создаваемых в terraform модуле, могут быть уже заняты. При помощи **sa_prefix** вы можете определить дополнительный префикс, добавляемый к именам сервисных аккаунтов, чтобы сделать их уникальными. По умолчанию равен пустой строке.
    - **db_user** - имя пользователя базы данных. По умолчанию: *"bingo"*.
    - **db_name** - название базы данных. По умолчанию: *"bingo"*.
    - **ssh_key** - путь к публичному SSH ключу, для SSH доступа на созданные виртуальные машины. По умолчанию: *"~/.ssh/id_rsa.pub"*.

Если вы используете [Yandex Cloud CLI](https://cloud.yandex.ru/docs/cli/), то вы можете воспользоваться вспомогательным скриптом:

```
source scripts/prepare_env.sh
```

Этот скрипт постарается извечь большинство необходимых значений из профиля Yandex Cloud CLI (задействуются переменные окружения **TF_VAR_***).

Когда `terraform apply` завершит свою работу, в разделе `Outputs` вы увидите два IP адреса:
- **service_ip** - адрес, по которому доступен развернутый сервис.
- **db_ip** - адрес, по которому доступна база данных. Бывает нужен, чтобы следить за процессом инициализации базы данных.

Также вы увидите **registry_id** - который может понадобится, если вы будете пользоваться Yandex Cloud CLI или вспомогательными скриптами.

Инициализация базы данных - это продолжительный процесс, во время которого сервис недоступен. Чтобы следить за процессом наполнения базы данных, можно подключиться по ssh на **db_ip** пользователем **ubuntu** и использовать команду:

```
docker logs --follow postgres
```

## Использование доменного имени

Эту часть решения трудно автоматизировать, не зная, какой DNS провайдер будет использоваться. Поэтому было решено оставить эту часть настройки ручной. Я использовал для экспериментов сервис [No-IP](https://www.noip.com/).

## Поддержка HTTPS

Данный terraform модуль автоматически сгенерирует и подключит самоподписанный сертификат.

Также сервис снабжен скриптами, облегчающими получение [Let's Encrypt](https://letsencrypt.org/) сертификата. Убедитесь, что DNS имя (параметр **dns_name** terraform модуля) корректно резолвится в **service_ip**, после чего подключитесь по ssh на **service_ip** пользователем **ubuntu** и выполните:

```
certbot-init <email>
```

где <*email*> - это адрес электронной почты, на который вы хотите получать уведомления об истечении срока сертификата. `certbot-init` автоматически добавит задачу `cron`, чтобы ежемесячно продлевать сертификат.

Я не стал автоматизировать выпуск Let's Encrypt сертификата, так как эта задача зависит от корректной настройки DNS, которая должна быть выполнена отдельно.

## Поддержка HTTP/3

Сервис поддерживает протокол HTTP/3, однако браузеры не будут использовать его на самоподписанном сертификате. Для полноценного использования HTTP/3 выпустите Let's Encrypt сертификат.

## Уничтожение тестового стенда

Перед удалением стенда, необходимо удалить все Docker-образы, загруженные в Container Regisry. Вы можете сделать это вручную. Если вы используете Yandex Cloud CLI, то вы можете воспользоваться вспомогательным скриптом:

```
./scripts/clear_docker_registry.sh <registry_id>
```

где <*registry_id*> - это ID Container Registry, который вы хотите очистить (вы увидите его в `Outputs` после `terraform apply`).

После очистки registry, стенд можно разобрать стандартной командой:

```
terraform destroy
```

## Пояснения к решению

### Балансировщик нагрузки

В качестве балансировщика нагрузки было решено использовать [HAProxy](https://www.haproxy.org/), потому что он из коробки хорошо работает с active health check.

В качестве кеширующего сервера и SSL-терминатора используется тот же HAProxy, потому что он умеет.

Для поддержки HTTP/3 была выбрана специальная сборка HAProxy с поддержкой QUIC (<https://hub.docker.com/r/haproxytech/haproxy-ubuntu-quic>). Использование QUIC в HAProxy требует явного указания IP адреса интерфейса, на котором запустится listener, поэтому контейнер использует статическую адресацию (см. `configs/alb/docker-compose.yaml.tftpl`).

### Let's Encrypt

Для выпуска Let's Encrypt сертификата используется [certbot](https://certbot.eff.org/). HAProxy с поддержкой QUIC распространяется в виде Docker контейнера, поэтому certbot было решено использовать также через Docker контейнер (<https://hub.docker.com/r/certbot/certbot>). Чтобы пройти ACME проверки, certbot должен иметь возможность ответить на 80 порту в Интернет. Так как этот порт на хосте уже занят HAProxy, пришлось настроить в нем отдельный бэкэнд для ACME проверок, который перенаправляет запросы на контейнер с standalone certbot сервером. Так как контейнер с standalone certbot сервером не запущен постоянно, пришлось выделить для него статический IP адрес. Этот IP адрес внесен в файл конфигурации HAProxy, и контейнер certbot всегда запускается с этим адресом. Эти подробности запуска certbot были спрятаны в два скрипта: `certbot-init` и `certbot-renew` (лежат в `configs/alb`).

### Приложение bingo

Приложение обернуто в Docker-контейнер и запускается через docker compose (`app/server.Docker` и `configs/app/docker-compose.yaml.tftpl`). Перезапуск упавшего приложения выполняется средствами docker compose. Иногда приложение завершается с кодом 2, что мешает docker compose автоматически перезапустить его, поэтому в контейнере запуск bingo завернут в shell команду, которая принудительно меняет любой ненулевой код завершения на 1. Иногда bingo не завершается, но начинает постоянно отдавать 5xx ошибки. За таким поведением следит отдельная cron задача, которая принудительно перезапускает контейнер (`configs/app/cron_bingo_monitor`). Log файлы bingo доступны на хосте в директории `/var/log/bingo` и управляются logrotate.

### Инициализация базы данных

В решении используется Docker образ postgresql, дооснащенный приложением bingo (`app/postgres.Dockerfile`). Этот образ имеет стандартный механизм инициализации базы данных через `/docker-entrypoint-initdb.d`. Первоначально `bingo prepare_db` наполняет базу данными (`configs/db/bingo_prepare_db.sh`). Выяснилось, что `bingo prepare_db` не создает в базе никаких индексов, даже первичных ключей, поэтому после него запускается sql файл, который их добавляет (`configs/db/fix_tables_keys.sql`).

